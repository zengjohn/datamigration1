## 冒烟测试简单流程



### 执行环境

`Jdk21`

`mysql 8.0`

`nginx` (可选)



### 迁移机器

假设有三台迁移机器, 

| 迁移机器  | `IP`地址       | `csv`数据目录          | `迁移生产文件目录`   |
| --------- | -------------- | ---------------------- | -------------------- |
| 迁移机器1 | 192.168.37.101 | `/root/mig1/test_data` | `/root/mig1/out_dir` |
| 迁移机器2 | 192.168.37.102 | 同上                   | 同上                 |
| 迁移机器3 | 192.168.37.103 | 同上                   | 同上                 |

说明：`csv数据目录`和迁移生产文件目录在同一次迁移作业中， 各个迁移机器必须一样



**元数据库 `mysql 8`** , `jdbc url`地址 `jdbc:mysql://192.168.37.101:3306/test2`



**开放端口**

迁移机器互相开放 `server.port` 

迁移机器向`nginx`(如果有)开放`server.port`端口

`nginx`机器(如果有)开放监听端口供浏览器访问



### 清理演示环境

元数据库`mysql8`

```sql
drop table if exists `test1`.`csv_split`;
drop table if exists `test1`.`qianyi_detail`;
drop table if exists `test1`.`qianyi`;
drop table if exists `test1`.`migration_job`;
```



目标库tdsql

```mysql
drop table if exists `test_iconv`;
drop table if exists `test_ibm1388`;
drop table if exists `test_bigtable`;
```



### 修改配置文件

`application.yml`

| 配置                  | 含义              | 解释                     |
| --------------------- | ----------------- | ------------------------ |
| `spring.datasource`   | 元数据库          | 用于保存迁移程序管理信息 |
| `app.current-node-ip` | 迁移机器真实`ip`  | 每台机器自己的`ip`地址   |
| `app.csv`             | `csv`文件格式配置 |                          |



### 程序安装

安装`jdk21`

加压安装包

安装`nginx` （可选)

`nginx`反向代理用于统一前端, 

```
upstream migration_cluster {
    # 简单的轮询，或者 ip_hash
    server 192.168.1.101:8080;
    server 192.168.1.102:8080;
    server 192.168.1.103:8080;
    server 192.168.1.104:8080;
    server 192.168.1.105:8080;
}

server {
    listen 80;
    server_name migration.internal;

    location / {
        proxy_pass http://migration_cluster;
        proxy_set_header Host $host;
        # 开启 WebSocket 支持 (如果有实时进度推送)
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
    }
}
```



### 程序启停

启动

```bash
/root/mig1/etl-1.0.0# ./shell/start.sh
```



查看日志 `tail -f logs/app.log`



停止

```bash
/root/mig1/etl-1.0.0# ./shell/stop.sh
```



### 在目标库建表

目标表需要添加两个附加列，用于方便数据装载和比对。

`csv_id`: 该行数据从拆分`csv`文件中装载(在元数据表`csv_split`的id)

`source_row_no`: 该行数据在原`csv`(`ibm1388`编码)中的行数



迁移程序不会自动建表(`mysql`库)

```mysql
CREATE TABLE `test2`.`test_iconv` (
  `id` int(11) DEFAULT NULL,
  `name` varchar(64) DEFAULT NULL,
  `remark` varchar(32) DEFAULT NULL,
  `csv_id` int not null,
  `source_row_no` bigint(20) not null,
  KEY `idx_csvid_rowno` (`csv_id`,`source_row_no`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;

 CREATE TABLE `test2`.`test_ibm1388` (
  `user_id` varchar(64),
  `user_name` varchar(16300) DEFAULT NULL,
  `balance` decimal(10,2) DEFAULT NULL,
  `csv_id` int not null,
  `source_row_no` bigint(20) not null,
  KEY `idx_csvid_rowno` (`csv_id`,`source_row_no`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;

 CREATE TABLE `test2`.`test_bigtable` (
  `id` int(11) not null,
  `name` varchar(100) comment '姓名',
  `salary` decimal(19,2) comment '工资',
  `high` float comment '身高',
  `weight` double comment '体重',
  `birth1` date comment '出生日期',
  `birth2` datetime(6) comment '出生时间',   
  `luchAt` time comment '午饭时间',
  `remark` text comment '备注',            
  `csv_id` int not null,
  `source_row_no` bigint(20) not null,
  KEY `idx_csvid_rowno` (`csv_id`,`source_row_no`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;
```



说明：

1. 建表需要在原来表的基础上添加2个字段和一个索引

   `csv_id` 目标行对于的拆分文件(拆分表`csv_split`记录id)

   `source_row_no` 目标行在原 `csv`文件中的行数

2. 实际的分布式`tdsql`环境，需要给分片键。分片键选择不考虑业务场景，只考虑同步性能。选择`csvid`做分片键，这样同一个拆分`csv`文件数据(`load data infile`)都加载到同一个分片中。

3. `tdsql`分布式环境分片表可能不支持`load data infile`, 需要配置用普通的batch insert.

实际的`tdsql`环境建表语句

```sql
CREATE TABLE `test_iconv` (
  `id` int(11),
  `name` varchar(64),
  `remark` varchar(32),
  `csv_id` int  not null,
  `source_row_no` bigint(20) not null,
  primary key `idx_csvid_rowno` (`csv_id`,`source_row_no`)
) shardkey=`csv_id` ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;


 CREATE TABLE `test_ibm1388` (
  `user_id` varchar(64),
  `user_name` varchar(16300) DEFAULT NULL,
  `balance` decimal(10,2) DEFAULT NULL,
  `csv_id` int not null,
  `source_row_no` bigint(20) not null,
  primary key `idx_csvid_rowno` (`csv_id`,`source_row_no`)
) shardkey=`csv_id` ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;

 CREATE TABLE `test_bigtable` (
  `id` int(11) not null,
  `name` varchar(100) comment '姓名',
  `salary` decimal(19,2) comment '工资',
  `high` float comment '身高',
  `weight` double comment '体重',
  `birth1` date comment '出生日期',
  `birth2` datetime(6) comment '出生时间',   
  `luchAt` time comment '午饭时间',
  `remark` text comment '备注',
  `csv_id` int not null,
  `source_row_no` bigint(20) not null,
  primary key `idx_csvid_rowno` (`csv_id`,`source_row_no`)
) shardkey=`csv_id` ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;
```



说明：选择`csv_id`分片键应该有比较好的装载性能，如果迁移完成后要删除附加列，则分片键不能选择附加字段，需要选择业务字段。



### 界面新加迁移作业

如果有`nginx`统一前端，登录`nginx`， 否则登录任何一个迁移机器的前端

登录：http://192.168.37.101:8080/



#### 新建迁移作业

```
作业名称：用户表迁移
监控目录: /root/mig1/test_data
输出目录: /root/mig1/out_dir
目标数据库URL： jdbc:mysql://192.168.37.101:3316/test2
```



说明：

 1. 一个迁移作业只需要创建一个迁移作业，并不需要为每台机器创建一个迁移作业

 2. 同一个迁移作业，监控目录和输出目录， 各个迁移机器上都要有并且相同

    

#### 创建迁移请求

说明：各个迁移机器，各个独立的创建迁移请求。



上传`csv`文件到迁移目录

1) 上传 `test_iconv.csv` 到迁移机器

2) 上传 表结构定义文件 `test_iconv.sql` 到迁移机器

   示例表结构文件 `test_iconv.sql`

   文本格式，每一行是表的列名和类型，目前只用到列名

   ```
   id,number
   name,varchar
   remark,varchar
   ```

   

3) 上传`ok`文件 `test_iconv.ok`

   示例`test_iconv.ok`文件

   ```json
   {
     "ddl": "test_iconv.sql",
     "csv": [
         "test_iconv.csv",
         "test_iconv1.csv"
     ]
   }
   ```
   
   文件格式说明， `json`格式
   

`ddl`指定表结构定义文件（全路径)

`csv`定义数据文件全路径（可以有多个）



`ok`文件上传后，迁移自动开始



可以继续上传其他迁移请求

比如上传`csv`文件   `test_ibm1388.csv` , 上传表结构定义 `test_ibm1388.sql`, 然后上传`ok`文件 `test_ibm1388.ok`

`test_ibm1388.sql`

```
user_id,number
user_name,varchar
balance,decimal
```



`test_ibm1388.ok`

```json
{
  "ddl": "test_ibm1388.sql",
  "csv": [
      "test_ibm1388.csv"
  ]
}
```



大数据量

`test_bigtable.sql`

```
id,number
name,varchar
salary,decimal
high,float
weight,double
birth1,date
birth2,datetime
luchAt,time
remark,text
```



`test_bigtable.ok`

```
{
  "ddl": "test_bigtable.sql",
  "csv": [
      "test_bigtable.csv"
  ]
}
```



目标端表名约定

​	1. 根据`sql`文件名推断目标端表名， `目标库-目标表.sql`。

​	2. 如果`ok`文件中定义有 `schema`字段，则最终库名为 `ok`文件的`schema`字段的值。

​	3. 如果`ok`文件中定义有 `table`字段，则最终表名为 `ok`文件的`table`字段的值。

比如：

​	`sql`文件名  `test_bigtable.sql`, 表示目标表名为 `test_bigtable`, 目标库名为`jdbc`连接的库。

​	`sql`文件名  `john1-test_bigtable.sql`, 表示目标表名为 `test_bigtable`, 目标库名为`john1`。





#### 监看迁移过程

在界面查看迁移过程



### `application.yml`配置说明



| 配置项              |                    |                         |                | 含义                                                         | 说明         | 默认值                              |
| ------------------- | ------------------ | ----------------------- | -------------- | ------------------------------------------------------------ | ------------ | ----------------------------------- |
| `server.port`       |                    |                         |                | 监听端口                                                     |              |                                     |
| `spring.datasource` |                    |                         |                | 元数据库                                                     |              |                                     |
|                     | `url`              |                         |                | 元数据库`url`                                                |              |                                     |
|                     |                    |                         |                |                                                              |              |                                     |
|                     | `hikari`           |                         |                | 元数据库连接池配置                                           |              |                                     |
|                     |                    |                         |                |                                                              |              |                                     |
| `app`               |                    |                         |                | 应用特殊配置                                                 |              |                                     |
|                     | `current-node-ip`  |                         |                | 当前机器`ip`地址                                             | 必须真实准确 |                                     |
|                     | `job`              |                         |                |                                                              |              |                                     |
|                     |                    | `autoCleanOnClose`      |                | 关批次(`ok`)单时是否自动清除删除产生的输出文件, true表示自动删除 |              | true                                |
|                     | `csv`              |                         |                | `csv文件格式配置`                                            |              |                                     |
|                     |                    | `ibm-source`            |                | 源端`ibm csv`配置                                            |              |                                     |
|                     |                    | `utf8-split`            |                | 中间`csv`拆分文件配置                                        |              |                                     |
|                     | `performance`      |                         |                | 性能配置                                                     |              |                                     |
|                     | `executor`         |                         |                | 线程次配置                                                   |              |                                     |
|                     |                    | `transcode`             |                | 转码                                                         |              |                                     |
|                     |                    |                         | core-size      |                                                              |              |                                     |
|                     |                    |                         | max-size       |                                                              |              |                                     |
|                     |                    |                         | queue-capacity |                                                              |              |                                     |
|                     |                    | `load`                  |                | 装载,配置项同`transcode`                                     |              |                                     |
|                     |                    | `verify`                |                | 验证，配置项同`transcode`                                    |              |                                     |
|                     | `target-db-config` |                         |                | 目标库连接池                                                 |              |                                     |
|                     |                    | `max-pool-size`         |                |                                                              |              |                                     |
|                     |                    | `min-idle`              |                |                                                              |              |                                     |
|                     |                    | `connection-timeout`    |                |                                                              |              |                                     |
|                     |                    | `auto-commit`           |                | 必须配置成false, 关闭自动提交                                |              |                                     |
|                     |                    | `max-lifetime`          |                |                                                              |              |                                     |
|                     | `transcode`        |                         |                | 转码配置                                                     |              |                                     |
|                     |                    | `max-error-count`       |                | 最大失败错误， 超过则停止转码                                |              |                                     |
|                     | `load-jdbc`        |                         |                | 装载`jdbc`配置                                               |              |                                     |
|                     |                    | `pre-sql-list`          |                | 预执行`sqls`                                                 |              |                                     |
|                     |                    | `useLocalInfile`        |                | true: 使用load data `infile`使用极速模式(正在快), false: 使用通用 `JDBC` Batch Insert 模式 (保底, 慢)。默认false |              | false                               |
|                     |                    | `batchSize`             |                | `JDBC` Batch 大小                                            |              | 5000                                |
|                     |                    | `maxRetries`            |                | 最大重试次数                                                 |              | 3                                   |
|                     |                    | `queryTimeout`          |                | 查询超时                                                     |              | 600(秒)                             |
|                     |                    | `columnNameCsvId`       |                |                                                              |              | 装载表附加列(记录`csv`拆分id)       |
|                     |                    | `columnNameSourceRowNo` |                |                                                              |              | 装载表附加列(记录数据在`csv`中的行) |
|                     | `verify`           |                         |                | 验证配置                                                     |              |                                     |
|                     |                    | `strategy`              |                | 验证策略 ， `USE_SOURCE_FILE`: 源文件和`tdsql`数据进行比对, `USE_UTF8_SPLIT`用拆分文件和`tdsql`数据进行验证 |              | USE_SOURCE_FILE                     |
|                     |                    | `maxDiffCount`          |                | 最大差异行, 超过后结束比对                                   |              | 1000                                |
|                     |                    | `deleteSplitVerifyPass` |                | 验证通过后是否支持删除中间输出文件                           |              | false                               |
| `logging`           |                    |                         |                | 日志配置                                                     |              |                                     |
|                     | `level`            |                         |                |                                                              |              |                                     |



**目标库连接池配置**

| **配置项**                 | **推荐值 (JDBC模式)** | **推荐值 (Load Data 模式)** | **说明**                                         |
| -------------------------- | --------------------- | --------------------------- | ------------------------------------------------ |
| `Spring 线程池`            | `core=50, max=60`     | `core=10, max=10`           | 决定了处理任务的并发速度                         |
| `JDBC 连接池 (Hikari)`     | `max=60`              | `max=10`                    | **必须 <= 线程池 Max**，作为保护数据库的最后防线 |
| `Split 文件大小`           | `10MB` (约 5-10万行)  | `50MB` (约 20-50万行)       | `JDB`C 模式文件小一点，重试成本低                |
| `rewriteBatchedStatements` | `true`                | N/A                         | `JDBC` 模式的**核心命门**，不开慢 10 倍          |



### 大机`CSV`文件格式配置

解析 **`IBM1388 (EBCDIC 简体中文)`** 格式的大机 `CSV` 文件，使用 `univocity-parsers` 时，核心痛点通常在于**编码兼容性**、**特殊控制符**以及**不定长字段的缓冲区溢出**， 以下是需要特别注意的配置。配置文件 `application.yml`,  `app.csv.ibm-source`。

```yaml
app:
  csv:
    ibm-source:
      # 【核心 1】编码设置
      # 必须使用 ICU4J 支持的标准名称。JDK 原生可能叫 "IBM1388"，但 ICU 常用 "x-IBM1388" 或 "cp1388"
      # 如果您的环境里有生僻字问题，确保 pom.xml 引入了 icu4j-charset
      encoding: IBM1388

      # 【核心 2】生僻字/转义支持
      # 对应您代码 TranscodeService 中的逻辑。
      # 如果大机生成文件时，将无法显示的字转义成了 \ABC\ 格式，这里必须开启，否则会原样读出。
      tunneling: true

      # 【核心 3】缓冲区防御
      # 大机导出的 CSV 有时某个备注字段特别长（例如 COBOL 定义了 PIC X(20000)）
      # 默认的 4096 很容易报 TextParsingException，建议给到 50000 或更大
      max-chars-per-column: 50000

      # 【核心 4】换行符
      # 推荐 "AUTO"。大机文件经过 FTP 传输到 Linux/Windows 后，换行符可能是 \n 也可能是 \r\n。
      # Univocity 的自动检测非常准，硬编码反而容易错。
      line-separator: "AUTO"

      # 【细节 1】去空格
      # 大机字段常带有定长补空的空格（Padding），读取时建议自动去除
      ignore-leading-whitespaces: true
      ignore-trailing-whitespaces: true

      # 【细节 2】空值处理
      # 如果 CSV 里写的是 "null" 字符串或空串，转换成真正的 Java null
      null-value: ""

      # 【细节 3】分隔符
      # 标准是逗号。但如果大机数据里包含逗号且没有正确转义，可以考虑在大机端改用生僻字符（如 | 或 \t）做分隔
      delimiter: ","
      quote: "\""
      quote-escape: "\""
```



#### 配置项深度解析

##### 1. `encoding: "x-IBM1388"`

- **背景**：Java 原生的 `IBM1388` 支持可能不全，或者在某些 JDK 版本（如 OpenJDK）中缺失。您的项目引入了 `icu4j`，使用 `x-IBM1388` 是调用 ICU 库最稳妥的方式。
- **注意**：确保 `CharsetFactory.java` 中的逻辑能优先加载 `JDK`，失败后回退到 ICU，或者直接指定由 ICU 加载。

##### 2. `tunneling: true`

- **背景**：`IBM1388` 字符集比 `GBK/UTF-8` 小，很多生僻字（如“𬱖”）在大机上无法直接存储或显示。
- **配合**：这个配置开关对应您 `TranscodeService.java` 中的逻辑。开启后，程序会调用 `FastEscapeHandler.unescape()`，自动识别并还原类似 `\2CC56\` 这样的 Hex 转义序列，将其变为真正的 UTF-8 字符入库。如果不开启，这些字就会变成乱码或原始的转义串。

##### 3. `max-chars-per-column: 50000`

- **痛点**：`univocity-parsers` 为了极速性能，默认复用一个较小的 `char[]` 缓冲区（默认 4096）。
- **风险**：大机数据导出时，常常包含巨大的 XML 报文或 Base64 字符串作为单一字段。一旦超过限制，程序会直接抛出 `TextParsingException` 导致任务中断。建议设为业务最大字段长度的 1.5 倍。

##### 4. `line-separator: "AUTO"`

- **原理**：Univocity 会读取文件前 `4KB` 数据，分析 `\r` 和 `\n` 的分布来自动决定换行符。
- **优势**：这让您的程序能同时兼容从 FTP (Binary模式) 下来的原始文件和 FTP (ASCII模式) 转换过的文件，无需运维人员手动改配置。



### 测试文件产生

#### 用`iconv`产生

`test_iconv.csv`

先创建一个普通的 `UTF-8 文件` `source.csv`

```
1,张三,Testing
```

执行转换命令

```bash
$ iconv -f UTF-8 -t IBM1388 source.csv -o test_iconv.csv
```



#### 用icu4j库产生

`test_ibm1388.csv`

```java
package com.example.moveprog;

import java.io.*;
import java.nio.file.Files;
import java.nio.file.Paths;
import java.util.Random;

public class TestDataGenerator {
    // 基础配置
    private static final String OUT_DIR = "testdata";
    // 您的项目使用的是 ICU4J 的 x-IBM1388
    private static final String CHARSET_IBM = "x-IBM1388";
    private static final String CHARSET_UTF8 = "UTF-8";

    public static void main(String[] args) throws Exception {
        try {
            Files.createDirectories(Paths.get(OUT_DIR));
            System.out.println(">>> 开始生成测试数据，输出目录: " + OUT_DIR);

            generateBasic();
            generateCategoryA_Basic();
            generateCategoryB_CsvFormat();
            generateCategoryC_BusinessRules();
            generateCategoryD_Performance(100_000); // 生成10万行做演示，可改为 10_000_000
            generateCategoryE_Exceptions();
            generateCategoryF_Tunneling(); // 【新增】针对 FastEscapeHandler

            System.out.println(">>> 所有测试文件生成完毕！");

        } catch (Exception e) {
            e.printStackTrace();
            System.err.println("生成失败，请检查是否引入了 icu4j 依赖");
        }
    }

    private static void generateBasic() throws IOException {
        String filename = OUT_DIR + "/test_ibm1388.csv";
        // 1. 模拟数据：包含普通中文、英文、以及"生僻字转义符"(测试 Tunneling 功能)
        // 对应 FastEscapeHandler 的逻辑: \2CC56\ 会被还原
        StringBuilder sb = new StringBuilder();
        //sb.append("user_id,user_name,balance\n");
        sb.append("1001,张三,100.00\n\n");
        sb.append("1002,李四_普通,200.50\n");
        sb.append("1003,王\\2CC56\\五,888.88"); // 测试转义: \2CC56\
        
        writeWithEncoding(filename, content, CHARSET_IBM);
        System.out.println("[A] 基础编码文件生成: " + filename);
    }

    // ==========================================
    // A. 基础与编码验证
    // ==========================================
    private static void generateCategoryA_Basic() throws IOException {
        String filename = OUT_DIR + "/A_Basic_Encoding.csv";
        // 包含：英文、普通中文、数字
        StringBuilder sb = new StringBuilder();
        //sb.append("user_id,user_name,balance\n");
        sb.append("A001,John Doe,100.00\n");
        sb.append("A002,张三_测试,200.50\n");
        sb.append("A003,李四_End,999.99");

        writeWithEncoding(filename, sb.toString(), CHARSET_IBM);
        System.out.println("[A] 基础编码文件生成: " + filename);
    }

    // ==========================================
    // B. 数据格式边界验证 (CSV 解析健壮性)
    // ==========================================
    private static void generateCategoryB_CsvFormat() throws IOException {
        String filename = OUT_DIR + "/B_Csv_Format_Edge.csv";
        StringBuilder sb = new StringBuilder();
        //sb.append("user_id,user_name,balance\n");

        // 1. 字段含分隔符 (需引号包裹)
        sb.append("B001,\"Doe, John\",100.00\n");

        // 2. 字段含换行符 (Univocity 需支持多行模式)
        sb.append("B002,\"张三\n换行测试\",200.00\n");

        // 3. 字段含引号 (需双重转义) -> 实际显示为: 王"五"
        sb.append("B003,\"王\"\"五\"\"\",300.00\n");

        // 4. 空字段与空行
        sb.append("B004,,0.00\n"); // user_name 为空
        sb.append(",,\n");         // 全空行

        // 5. 超长字段 (15KB 字符串)
        String longText = generateRandomString(15000);
        sb.append("B005,\"Long_Start_").append(longText).append("_End\",500.00\n");

        writeWithEncoding(filename, sb.toString(), CHARSET_IBM);
        System.out.println("[B] CSV格式边界文件生成: " + filename);
    }

    // ==========================================
    // C. 业务规则边界验证
    // ==========================================
    private static void generateCategoryC_BusinessRules() throws IOException {
        String filename = OUT_DIR + "/C_Business_Rules.csv";
        StringBuilder sb = new StringBuilder();
        //sb.append("user_id,user_name,balance\n");

        // 1. 金额极值
        sb.append("C001,Max_Value,9999999999.99\n");
        sb.append("C002,Min_Value,-9999999999.99\n");
        sb.append("C003,High_Precision,0.123456789\n"); // 看看数据库 Decimal(10,2) 是否报错或截断

        // 2. 空格敏感 (前导/后导空格)
        sb.append("C004,  Space_Test  ,100.00\n");

        // 3. 特殊ID
        sb.append("0000,Zero_ID,0\n");

        writeWithEncoding(filename, sb.toString(), CHARSET_IBM);
        System.out.println("[C] 业务规则文件生成: " + filename);
    }

    // ==========================================
    // D. 批量与性能验证
    // ==========================================
    private static void generateCategoryD_Performance(int rows) throws IOException {
        String filename = OUT_DIR + "/D_Performance_" + rows + ".csv";
        // 使用 BufferedWriter 避免内存溢出
        try (BufferedWriter writer = new BufferedWriter(
                new OutputStreamWriter(new FileOutputStream(filename), CHARSET_IBM))) {

            //writer.write("user_id,user_name,balance\n");
            Random rand = new Random();

            for (int i = 0; i < rows; i++) {
                // 模拟真实分布：大部分是短数据，偶尔有长数据
                String name = (i % 100 == 0) ? "Name_" + generateRandomString(500) : "User_" + i;
                String line = String.format("D%07d,%s,%.2f\n", i, name, rand.nextDouble() * 10000);
                writer.write(line);
            }
        }
        System.out.println("[D] 性能测试文件生成 (" + rows + "行): " + filename);
    }

    // ==========================================
    // E. 异常与容错验证 (二进制破坏)
    // ==========================================
    private static void generateCategoryE_Exceptions() throws IOException {
        // E1. 错误编码混杂 (IBM1388 中混入 UTF-8)
        File f1 = new File(OUT_DIR + "/E1_Mixed_Encoding.csv");
        try (FileOutputStream fos = new FileOutputStream(f1)) {
            //fos.write("user_id,user_name,balance\n".getBytes(CHARSET_IBM));
            fos.write("E001,正常IBM1388,100\n".getBytes(CHARSET_IBM));
            fos.write("E002,我是UTF8乱入,200\n".getBytes(CHARSET_UTF8)); // 这里会产生乱码
            fos.write("E003,正常IBM1388,300\n".getBytes(CHARSET_IBM));
        }

        // E2. 错误引号 (Unclosed Quote) - 导致解析器吞掉后续所有行
        String f2 = OUT_DIR + "/E2_Unclosed_Quote.csv";
        StringBuilder sb = new StringBuilder();
        //sb.append("user_id,user_name,balance\n");
        sb.append("E004,\"Unclosed Quote Start,100.00\n"); // 缺右引号
        sb.append("E005,Should_Be_Swallowed,200.00\n");
        writeWithEncoding(f2, sb.toString(), CHARSET_IBM);

        // E3. 文件截断 (写入一半突然结束)
        File f3 = new File(OUT_DIR + "/E3_Truncated.csv");
        try (FileOutputStream fos = new FileOutputStream(f3)) {
            //fos.write("user_id,user_name,bal".getBytes(CHARSET_IBM)); // Header都没写完
        }

        System.out.println("[E] 异常容错文件生成: E1, E2, E3");
    }

    // ==========================================
    // F. [新增] 转义穿透 (Tunneling) 验证
    // ==========================================
    private static void generateCategoryF_Tunneling() throws IOException {
        String filename = OUT_DIR + "/F_Tunneling_Handler.csv";
        StringBuilder sb = new StringBuilder();
        //sb.append("user_id,user_name,balance\n");

        // 1. 标准生僻字转义 (假设 \2CC56\ 代表一个生僻字)
        // 期望：入库后不再包含 \2CC56\，而是对应的 Unicode 字符(如果转换支持) 或 保持原样(取决于配置)
        sb.append("F001,张\\2CC56\\三,100.00\n");

        // 2. 连续转义
        sb.append("F002,A\\2CC56\\B\\2CC57\\C,200.00\n");

        // 3. 假转义 (格式不对，FastEscapeHandler 应该忽略)
        sb.append("F003,普通斜杠\\ABC,300.00\n");

        // 4. 边界情况：行尾转义不闭合
        sb.append("F004,未闭合\\2CC56,400.00\n");

        writeWithEncoding(filename, sb.toString(), CHARSET_IBM);
        System.out.println("[F] 转义穿透测试文件生成: " + filename);
    }

    // --- 工具方法 ---
    private static void writeWithEncoding(String path, String content, String encoding) throws IOException {
        File file = new File(path);
        try (BufferedWriter writer = new BufferedWriter(
                new OutputStreamWriter(new FileOutputStream(file), encoding))) {
            writer.write(content);
            System.out.println("生成成功: " + path);
            System.out.println("文件大小: " + file.length() + " 字节");
        }
    }

    private static String generateRandomString(int length) {
        String chars = "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789";
        StringBuilder sb = new StringBuilder(length);
        Random random = new Random();
        for (int i = 0; i < length; i++) {
            sb.append(chars.charAt(random.nextInt(chars.length())));
        }
        return sb.toString();
    }

}
```



### 环境优化

#### 迁移程序配置优化

**5 台 128核 猛兽级机器 + 本地 SSD + TDSQL Proxy** 架构的终极性能调优配置。

我们将资源划分为三个战场：

1. **CPU 战场 (Transcode)**：拼的是计算和本地 IO。
2. **网络/带宽战场 (Load & Verify)**：拼的是 TDSQL 连接数和网络吞吐。
3. **后勤战场 (Meta DB)**：拼的是短平快的状态更新并发能力。

------

##### 1. 核心配置文件 `application.yml`

请直接参考此配置进行调整。

YAML

```
spring:
  # ==========================================
  # 1. 元数据库 (Metadata DB) - 极速状态更新
  # ==========================================
  datasource:
    # 基础配置略 (url, username...)
    hikari:
      pool-name: MetaHikariPool
      # 【核心策略】元库只做状态更新(UPDATE status=RUNNING)，耗时极短(ms级)。
      # 即使有 100 个线程在跑任务，30 个连接足够轮转过来。
      # 不要设太大，以免 5 台机器总连接数(5*30=150)对 MySQL 造成压力。
      maximum-pool-size: 30
      minimum-idle: 10
      # 状态更新非常快，如果 30秒 拿不到连接说明 DB 挂了
      connection-timeout: 30000
      # 验证连接有效性，防止拿旧连接报错
      validation-timeout: 3000
      # 只有 meta 库需要检测连接泄漏，防止开发写代码忘了关连接
      leak-detection-threshold: 2000

app:
  # ==========================================
  # 2. 线程池配置 (各司其职)
  # ==========================================
  executor:
    # --- 转码 (CPU 密集型) ---
    transcode:
      # 128核 CPU，即使留出一半给系统和 GC，给 64 个核心跑计算也是安全的。
      # 这是一个纯 CPU + 本地 SSD 读写的操作，不涉及网络 IO，线程数≈核心数效率最高。
      core-size: 64
      max-size: 64
      # 队列稍微大点，作为缓冲
      queue-capacity: 500
      
    # --- 装载 (IO/网络 密集型) ---
    load:
      # 瓶颈在网络 RTT 和 TDSQL 写入速度。
      # 需要较多线程来填满网络等待时间。
      # 设定为 50，留出余量给 Verify。
      core-size: 50
      max-size: 50
      queue-capacity: 200

    # --- 验证 (IO/网络 密集型) ---
    verify:
      # 验证是 SELECT Count/Sum，速度通常比 Insert 快。
      # 给 20 个并发足够追平 50 个 Load 的产生速度。
      core-size: 20
      max-size: 20
      queue-capacity: 200

  # ==========================================
  # 3. 目标库连接池模板 (Target DB)
  # ==========================================
  target-db-config:
    # 【数学题】
    # Load线程(50) + Verify线程(20) = 70 并发。
    # 连接池必须 >= 70，否则线程拿不到连接会报错或空转。
    # 给 80 是为了留 10 个余量给临时查询或 Admin 操作。
    max-pool-size: 80
    
    # 保持一定的热连接，避免每次突发任务都要三次握手
    min-idle: 20
    
    # 目标库可能会因为繁忙响应慢，稍微容忍久一点的等待
    connection-timeout: 60000 
    
    # JDBC Batch 必须关闭自动提交
    auto-commit: false
    
    # 一个连接最长存活 30 分钟，防止服务端因为 wait_timeout 踢掉
    max-lifetime: 1800000
```

------

##### 2. 配套 Java 配置代码

为了让上面的 YAML 生效，我们需要更新 `AsyncConfig` 和 `AppProperties`。

###### A. 线程池配置 `AsyncConfig.java`

这里我们明确把三个业务的线程池分开，互不干扰。

###### B. 属性映射类 `AppProperties.java`

##### 3. 性能调优的数学逻辑（Why?）

**Q1: 为什么 Meta 库连接池 (30) < 线程总数 (64+50+20=134)?**

- **短事务特性**：Worker 线程只有在任务**开始前**（1ms）和**结束后**（1ms）才会用到 Meta 库更新状态。中间几秒钟的 CSV 解析、网络传输时间，它是**不占用** Meta 库连接的。
- 30 个连接足够应付 134 个线程的“瞬间请求”。

**Q2: 为什么 Target 库连接池 (80) ≈ 线程总数 (Load+Verify)?**

- **长事务特性**：Load 线程在执行 `executeBatch` 的整个过程中（假设 1秒），是**一直持有** Target 库连接的。
- 如果是 50 个 Load 线程同时跑，就必须实打实地占用 50 个物理连接。如果不匹配，线程就会排队等待连接，CPU 和带宽就浪费了。

**Q3: 为什么 Load 线程给 50？**

- 您有 5 台机器。
- **总并发** = 5 台 * 50 = **250 并发**。
- 对于 TDSQL Proxy 来说，250 个并发写入连接（开启了 `rewriteBatchedStatements`）已经是非常大的压力了。再大可能会导致 Proxy 响应变慢，得不偿失。

**Q4: JVM 内存建议**

- 既然开了这么多线程，且每个线程都要处理 10MB 级别的 CSV buffer。
- **堆内存 (Heap)**：建议给到 **16GB ~ 24GB** (`-Xms16g -Xmx16g`)。
- **垃圾回收 (GC)**：强烈建议使用 **G1 GC** (`-XX:+UseG1GC`)，因为大对象很多，G1 处理碎片化能力更好。
